{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mayaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012803077697753906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 7809,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7299e2f5e74d29a34e4f7714127ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013279199600219727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca874229d9340928897c04796f8a367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008551359176635742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 20979968,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e184f3b2554c26a7d0cbc27a4c8889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029148578643798828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 20470363,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdcb9d105434eb69fa98eb37b8a2f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028290510177612305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 41996509,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1637492b6f949ecb4129252614dd4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015888452529907227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb9e2d18c144943a6e7a84ec6c35090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03371024131774902,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 25000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784c9036308249029958ea7ad1bd7a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04077935218811035,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 25000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a2fe659ac242708ac01740a2d8a949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031707048416137695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating unsupervised split",
       "rate": null,
       "total": 50000,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ba8b2b17b84041a8815e4c88545931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Original): 0.852\n",
      "F1 Score (Original): 0.8509266720386786\n",
      "Classification Report (Original):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85      3752\n",
      "           1       0.86      0.85      0.85      3748\n",
      "\n",
      "    accuracy                           0.85      7500\n",
      "   macro avg       0.85      0.85      0.85      7500\n",
      "weighted avg       0.85      0.85      0.85      7500\n",
      "\n",
      "Accuracy (Porter Stemmer): 0.8473333333333334\n",
      "F1 Score (Porter Stemmer): 0.8473129750633418\n",
      "Classification Report (Porter Stemmer):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      3752\n",
      "           1       0.85      0.85      0.85      3748\n",
      "\n",
      "    accuracy                           0.85      7500\n",
      "   macro avg       0.85      0.85      0.85      7500\n",
      "weighted avg       0.85      0.85      0.85      7500\n",
      "\n",
      "Accuracy (Lancaster Stemmer): 0.8498666666666667\n",
      "F1 Score (Lancaster Stemmer): 0.8501862692921767\n",
      "Classification Report (Lancaster Stemmer):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      3752\n",
      "           1       0.85      0.85      0.85      3748\n",
      "\n",
      "    accuracy                           0.85      7500\n",
      "   macro avg       0.85      0.85      0.85      7500\n",
      "weighted avg       0.85      0.85      0.85      7500\n",
      "\n",
      "Accuracy (Snowball Stemmer): 0.8488\n",
      "F1 Score (Snowball Stemmer): 0.8488805970149255\n",
      "Classification Report (Snowball Stemmer):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      3752\n",
      "           1       0.85      0.85      0.85      3748\n",
      "\n",
      "    accuracy                           0.85      7500\n",
      "   macro avg       0.85      0.85      0.85      7500\n",
      "weighted avg       0.85      0.85      0.85      7500\n",
      "\n",
      "Accuracy (BPE Tokenization): 0.8422666666666667\n",
      "F1 Score (BPE Tokenization): 0.8427489033630201\n",
      "Classification Report (BPE Tokenization):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      3752\n",
      "           1       0.84      0.85      0.84      3748\n",
      "\n",
      "    accuracy                           0.84      7500\n",
      "   macro avg       0.84      0.84      0.84      7500\n",
      "weighted avg       0.84      0.84      0.84      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "data = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Basic preprocessing\n",
    "data['text'] = data['text'].str.lower()  # Convert text to lowercase\n",
    "data['label'] = data['label'].map({1: 1, 0: 0})  # Ensure labels are binary\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [porter.stem(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def lancaster_stemmer(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [lancaster.stem(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def snowball_stemmer(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [snowball.stem(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "data['porter_stemmed'] = data['text'].apply(porter_stemmer)\n",
    "data['lancaster_stemmed'] = data['text'].apply(lancaster_stemmer)\n",
    "data['snowball_stemmed'] = data['text'].apply(snowball_stemmer)\n",
    "\n",
    "with open(\"text_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for review in data['text']:\n",
    "        f.write(review + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(input='text_data.txt', model_prefix='bpe', vocab_size=5000, model_type='bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file='bpe.model')\n",
    "\n",
    "def bpe_tokenizer(text):\n",
    "    return \" \".join(sp.encode_as_pieces(text))\n",
    "\n",
    "data['bpe_tokenized'] = data['text'].apply(bpe_tokenizer)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Apply TF-IDF to original, stemmed, and BPE tokenized text\n",
    "X = tfidf.fit_transform(data['text']).toarray()\n",
    "X_porter = tfidf.fit_transform(data['porter_stemmed']).toarray()\n",
    "X_lancaster = tfidf.fit_transform(data['lancaster_stemmed']).toarray()\n",
    "X_snowball = tfidf.fit_transform(data['snowball_stemmed']).toarray()\n",
    "X_bpe = tfidf.fit_transform(data['bpe_tokenized']).toarray()\n",
    "\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_porter, X_test_porter, y_train_porter, y_test_porter = train_test_split(X_porter, y, test_size=0.3, random_state=42)\n",
    "X_train_lancaster, X_test_lancaster, y_train_lancaster, y_test_lancaster = train_test_split(X_lancaster, y, test_size=0.3, random_state=42)\n",
    "X_train_snowball, X_test_snowball, y_train_snowball, y_test_snowball = train_test_split(X_snowball, y, test_size=0.3, random_state=42)\n",
    "X_train_bpe, X_test_bpe, y_train_bpe, y_test_bpe = train_test_split(X_bpe, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"Accuracy (Original):\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score (Original):\", f1_score(y_test, y_pred))\n",
    "print(\"Classification Report (Original):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Porter Stemmed text\n",
    "nb_porter = MultinomialNB()\n",
    "nb_porter.fit(X_train_porter, y_train_porter)\n",
    "y_pred_porter = nb_porter.predict(X_test_porter)\n",
    "print(\"Accuracy (Porter Stemmer):\", accuracy_score(y_test_porter, y_pred_porter))\n",
    "print(\"F1 Score (Porter Stemmer):\", f1_score(y_test_porter, y_pred_porter))\n",
    "print(\"Classification Report (Porter Stemmer):\\n\", classification_report(y_test_porter, y_pred_porter))\n",
    "\n",
    "# Lancaster Stemmed text\n",
    "nb_lancaster = MultinomialNB()\n",
    "nb_lancaster.fit(X_train_lancaster, y_train_lancaster)\n",
    "y_pred_lancaster = nb_lancaster.predict(X_test_lancaster)\n",
    "print(\"Accuracy (Lancaster Stemmer):\", accuracy_score(y_test_lancaster, y_pred_lancaster))\n",
    "print(\"F1 Score (Lancaster Stemmer):\", f1_score(y_test_lancaster, y_pred_lancaster))\n",
    "print(\"Classification Report (Lancaster Stemmer):\\n\", classification_report(y_test_lancaster, y_pred_lancaster))\n",
    "\n",
    "# Snowball Stemmed text\n",
    "nb_snowball = MultinomialNB()\n",
    "nb_snowball.fit(X_train_snowball, y_train_snowball)\n",
    "y_pred_snowball = nb_snowball.predict(X_test_snowball)\n",
    "print(\"Accuracy (Snowball Stemmer):\", accuracy_score(y_test_snowball, y_pred_snowball))\n",
    "print(\"F1 Score (Snowball Stemmer):\", f1_score(y_test_snowball, y_pred_snowball))\n",
    "print(\"Classification Report (Snowball Stemmer):\\n\", classification_report(y_test_snowball, y_pred_snowball))\n",
    "\n",
    "# BPE Tokenized text\n",
    "nb_bpe = MultinomialNB()\n",
    "nb_bpe.fit(X_train_bpe, y_train_bpe)\n",
    "y_pred_bpe = nb_bpe.predict(X_test_bpe)\n",
    "print(\"Accuracy (BPE Tokenization):\", accuracy_score(y_test_bpe, y_pred_bpe))\n",
    "print(\"F1 Score (BPE Tokenization):\", f1_score(y_test_bpe, y_pred_bpe))\n",
    "print(\"Classification Report (BPE Tokenization):\\n\", classification_report(y_test_bpe, y_pred_bpe))\n",
    "\n",
    "# Cleanup temporary files\n",
    "os.remove(\"text_data.txt\")\n",
    "os.remove(\"bpe.model\")\n",
    "os.remove(\"bpe.vocab\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
